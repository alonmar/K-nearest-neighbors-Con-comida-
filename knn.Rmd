---
title: "R Notebook"
output:
  md_document:
    variant: markdown_github
  html_notebook: null
  html_document:
    df_print: paged
  pdf_document: default
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
```


En este tutorial explicare los conceptos básicos acerca del algoritmo K vecinos cercanos (k-Nearest Neighbor) o simplemente KNN.

Quizás este sea uno de los algoritmos mas sencillos en machine learning, de hecho muchos autores hacen llamar a este algoritmos como un algoritmo de **aprendizaje flojo**(lazy learning), ya que en un sentido estricto el algoritmo no esta aprendiendo, y tampoco ocurre una abstracción como tal, lo que sucede es que en la fase de entrenamiento simplemente almacena datos, por lo que no surge una abstracción del modelo, simplemente **memoriza** de tal manera que el aprendizaje sucede por memorización, haciendo muy lenta la fase de predicción.

Sin embargo es un algoritmo muy utilizado para la clasificación ya que tiene muy buenas características que ayudan a dar un primer vistazo a los datos e inclusive ser el algoritmo final.

Veamos algunas de sus fortalezas y debilidades

Fortalezas | Debilidades
-- | --
Simple y efectivo | Al no producir un modelo es difícil seguir como las variables se relacionan con las clases
No es muy necesario hacer suposiciones de los datos | Requiere una selección apropiada del valor K
Fase de entrenamiento rápida |  Fase de clasificación lenta
| | Variables nominales y datos faltantes requieren de un trato especial

##Definiendo el modelo

Muy bien ya explicamos las debilidades y bondades del algoritmo ahora toca explicar como funciona.

Imaginemos que deseamos clasificar una nueva observación (punto negro) y existen tres posibles clases

```{r,echo=FALSE}
df <- data.frame(x = c(3,1,1,5,6,8,7,8,9),
                 y = c(4,5,6,5,8,7,1,1,2),
                 clase = c("desconocido","a","a","b","b","b","c","c","c"))

p <- ggplot(df, aes(x, y, group=clase)) +
  geom_point(aes(shape = clase ,color=clase),size=4)+ylim(-1, 11)+
  scale_shape_manual(values=c(18, 15, 17, 16))+
  scale_color_manual(values=c('#dd5a5a','#5fce66', '#56B4E9', '#000000'))+
  xlim(-1,11)+theme(aspect.ratio = 1)
p
```

### Paso uno

Debemos calcular la distancia de este nuevo punto (observación) hacía todos los otros puntos que ya se encuentran etiquetados con su clase correspondiente para después ordenar de menor a mayor.


```{r, echo=FALSE}
p +
  geom_segment(data=df[-1,], mapping=aes(x=3, y=4, xend=x, yend=y), arrow = arrow( type = "closed",length = unit(0.1, "inches")),size=.5, lineend = "round")  
```

```{r,echo=FALSE}

dist <- function(point,data){

  sqrt((point[1]-data[,1])^2+(point[2]-data[,2])^2)
}
```

```{r,echo=FALSE}
distancias <- data.frame(dist=dist(point=c(3,4),df[-1,1:2]),
                          clase = df[-1,3])%>%
   arrange(dist)


knitr::kable(distancias)
```



### Paso dos

Ya que tenemos las distancias ordenadas de menor a mayor basta con tomar los mas cercanos al punto a clasificar, lo cual esta determinado con el valor K seleccionado, digamos que para este ejemplo el valor de K es 3

```{r,echo=FALSE}
distancias %>% head(3) %>%  knitr::kable()
```

### Paso tres

Ahora solo basta con observar la clase que mas se repite en los K vecinos seleccionados y !Felicidades! hemos clasificado la nueva observación.



```{r,echo=FALSE}
distancias %>% head(3) %>%
  group_by(clase) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%  knitr::kable()
```

En este ejemplo vemos que el que mas se repite es la clase _**a**_ por lo tanto nuestra figura pertenece a esta clase.

```{r,echo=FALSE}
df <- data.frame(x = c(3,1,1,5,6,8,7,8,9),
                 y = c(4,5,6,5,8,7,1,1,2),
                 clase = c("a","a","a","b","b","b","c","c","c"))

p <- ggplot(df, aes(x, y, group=clase)) +
  geom_point(aes(shape = clase ,color=clase),size=4)+ylim(-1, 11)+
  scale_shape_manual(values=c(18, 15, 17, 16))+
  scale_color_manual(values=c('#dd5a5a','#5fce66', '#56B4E9', '#000000'))+
  xlim(-1,11)+theme(aspect.ratio = 1)
p
```

### Distancias

Es importante mencionar que para el ejemplo anterior la forma en la que medimos las distancias fue mediante la distancia Euclidiana

$\sqrt{\sum_{i=1}^{m}(x_{i}-y_{i})^{2}}$

la cual es la mas utilizada sin embargo también se puede calcular mediante distancia Manhattan

$\sum_{i=1}^{m}\left | x_{i}-y_{i} \right |$

o mediante la distancia Hamming usualmente para identificar si un valor a cambiado o se mantiene igual

si $x=y$ entonces la distancia es igual a $0$

si $x\neq y$ entonces la distancia es igual a $1$

x | y | distancia
-- | -- |--|
perro | perro | 0|
perro | gato | 1|

### Valor de k

Optimizar el valor de K es de suma importancia, ya que al elegir un k-valor muy pequeño es posible que el ruido de los datos tome importancia, por otro lado si el valor de k es alto la clase que sea mayoría tendrá un peso importante.

A continuación explicare gráficamente lo anterior.

#### k-valor pequeño (overfitting)

Imaginemos el siguiente caso:

```{r,echo=FALSE}
set.seed(11)
a <- data.frame(x = c(runif(15,5,10),2,2.5),
                      y = c(runif(15,5,10),2,1.5),
                      clase = "A")
b <- data.frame(x = runif(15,1,5),
                      y = runif(15,1,5),
                      clase = "B")

d <- data.frame(x = 2.5,
                      y = 2,
                      clase = "desconocido")

k_small <- rbind(a,b,d)
 ggplot(k_small, aes(x, y, group=clase)) +
  geom_point(aes(shape = clase ,color=clase),size=3)+ylim(-1, 11)+
  scale_shape_manual(values=c(18, 15, 16, 16))+
  scale_color_manual(values=c('#5faace','#775fce', '#000000', '#000000'))+
  xlim(-1,11)+theme(aspect.ratio = 1)
```

De elegir un k-valor de 3 la nueva observación será colocado en la clase _**A**_ aunque visualmente es evidente que pertenece a la clase _**B**_, sin embargo aquí es donde el **ruido toma importancia**, y se dice que el modelo esta sobre ajustado.

#### k-valor alto (underfitting)

```{r,echo=FALSE}

a <- data.frame(x = c(runif(20,5,10)),
                      y = c(runif(20,5,10)),
                      clase = "a")
b <- data.frame(x = runif(10,1,5),
                      y = runif(10,1,5),
                      clase = "b")

d <- data.frame(x = 2.5,
                      y = 2,
                      clase = "desconocido")

k_small <- rbind(a,b,d)
 ggplot(k_small, aes(x, y, group=clase)) +
  geom_point(aes(shape = clase ,color=clase),size=3)+ylim(-1, 11)+
  scale_shape_manual(values=c(18, 15, 16, 16))+
  scale_color_manual(values=c('#5faace','#775fce', '#000000', '#000000'))+
  xlim(-1,11)+theme(aspect.ratio = 1)
```

Por otra parte si tomamos el otro extremo donde k es igual al total de los datos, pasara el subajuste del algoritmo, ya que visualmente el punto desconocido pertenece a la clase _**B**_ sin embargo al ser la clase _**A**_ mayoría, es aquí donde será clasificado.

Al analizar los extremos (k muy pequeño y k muy alto) es clara la importancia de un k-valor adecuado, normalmente se escoge como un buen valor de inicio la raíz cuadrada del numero de datos para el entrenamiento, sim embargo es buena practica tomar varios valores de k, y observar cual de estos es el que nos da una mejor clasificación de los datos de entrenamiento.

# Clasificación de frutas

Ahora toca hacer un clasificar de alimentos usando el algoritmo KNN.

Crearemos los datos

```{r}
vegetales <- data.frame(nombre = c("zanahoria","apio","ejote","pepino","lechuga"),
                        dulzura = c(6,3,4,2,1),
                        crujiente = c(10,10,7,8,9),
                        tipo = "vegetal"
                        )

frutas <- data.frame(nombre = c("manzana","platano","uva","naranja","pera"),
                        dulzura = c(10,9,9,8,10),
                        crujiente = c(9,1,5,3,6),
                        tipo = "fruta"
                        )

proteina <- data.frame(nombre = c("tocino","queso","nuez","camarón","pescado"),
                        dulzura = c(1,1,3,2,3),
                        crujiente = c(4,1,5,3,2),
                        tipo = "proteina"
                        )

alimentos <- rbind(vegetales,frutas,proteina)

```

Como vemos tenemos tres clases(_tipo_) **vegetales, frutas y proteínas**, cada uno de los alimentos tiene una calificación que se encuentra entre 1 y 10, califican que tan crujientes y dulces son.

Gráficamente estarían representados de la siguiente manera
```{r,echo=FALSE}
plot <-  alimentos %>% ggplot(aes(dulzura,crujiente,colour = tipo)) +
   geom_text(aes(label=nombre))+
 ylim(-1, 11)+ xlim(-1,11)+theme(aspect.ratio = 1)
plot
```

Pero ¿Y un tomate? ¿A que clase pertenece? Veámoslo

```{r}

tomate <- data.frame(nombre = "tomate",
                        dulzura = 6,
                        crujiente = 4,
                        tipo = "desconocido"
                        )

```

```{r}
plot + geom_label(data=tomate, aes(dulzura,crujiente,label="tomate"))

```

Crearemos una función que nos ayudara a calcular la distancia que existe de un punto a todos los demás

```{r}
dist <- function(point,data){

  sqrt((point[1]-data[,1])^2+
       (point[2]-data[,2])^2)
}
```

Ahora calculemos todas las distancias a nuestro tomate y ordenémoslo
```{r}
distancias <- data.frame(nombre = alimentos$nombre,
                          tipo = alimentos$tipo,
                          dist=dist(point = c(6,4),data = alimentos[,2:3]))%>%
   arrange(dist)   

knitr::kable(distancias)
```

la siguiente función nos regresara la clase correspondiente al K valor seleccionado

```{r}
resultado <- function(distancias,k) {
  result <- head(distancias,k) %>%
    group_by(tipo) %>%
    summarise(n = n()) %>%
    arrange(desc(n))

  cat("Pertenece a" ,as.character(result$tipo[1]) )

}
```

```{r}
resultado(distancias,4)
```

Genial nuestro tomate es una fruta! :D


Ahora metamos todo en la función **_clasificador_** y veamos que sucede al clasificar un Piña.

```{r}
clasificador <- function(point,dataframe,k) {
  #Calculando distancias y ordenándolo
  distancias <- data.frame(nombre = alimentos$nombre,
                          tipo = alimentos$tipo,
                          dist=dist(point=point,alimentos[,2:3])) %>%
    arrange(dist)

  #Calsificando de acuerdo al k-valor
  resultado(distancias,k)
}


```

```{r}
clasificador(c(8,4),alimentos,4)
```

## Conclusiones

Como vimos quizás no es un algoritmo super elaborado, sin embargo es un algoritmo que es muy intuitivo y excelente para comenzar este lindo camino de la ciencia de datos y machine learning. Aquí se mostro como funciona si te gustaría profundizar más te invito a que investigues por que es importante tener variables estandarizadas.
